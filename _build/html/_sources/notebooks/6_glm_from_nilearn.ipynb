{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f786ec2",
   "metadata": {},
   "source": [
    "# 6. First level GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28579285",
   "metadata": {},
   "source": [
    "In this chapter, we will go through a simple workflow of the first level general linear modeling with a BIDS dataset from openneuro. This analysis is only performed on **one** subject.\n",
    "\n",
    "This tutorial is based on the [Nilearn GLM tutorial](https://nilearn.github.io/stable/auto_examples/04_glm_first_level/plot_bids_features.html#sphx-glr-auto-examples-04-glm-first-level-plot-bids-features-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8134eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2cc50a",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Import packages that will be used globally and set up output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ec37d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pydra\n",
    "from pydra import Workflow\n",
    "from pydra.engine.specs import File\n",
    "import typing as ty\n",
    "from pathlib import Path\n",
    "\n",
    "# get current directory\n",
    "pydra_tutorial_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "# set up output directory\n",
    "workflow_dir = Path(pydra_tutorial_dir) / \"outputs\" \n",
    "workflow_out_dir = workflow_dir / \"6_glm\"\n",
    "\n",
    "# create the output directory if not exit\n",
    "os.makedirs(workflow_out_dir, exist_ok = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e226c1",
   "metadata": {},
   "source": [
    "## Create tasks\n",
    "\n",
    "In this section, we converte major steps into tasks.\n",
    "Each pydra task can have multiple python functions. We recommand to put those logically more related functions into the same task.\n",
    "\n",
    "It is very **important** to keep in mind what adjacent tasks of your current task will be.\n",
    "1. Your previous task will decide your arguments in the current task\n",
    "2. Your next task will be impacted by the returns in the current task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7600e3aa",
   "metadata": {},
   "source": [
    "### fetch openneuro BIDS dataset\n",
    "\n",
    "In this task, we do the following:\n",
    "1. get openneuro dataset index\n",
    "2. specify exclusion patterns and number of subjects\n",
    "3. download the data we need\n",
    "\n",
    "\n",
    "**Notes:** Here we still use `n_subjects` as an argument. Given that we will only analyze one subject, you can also remove this argument and specify `n_subjects =1` in `select_from_index`. If you do, do not forget to modify the argument in the workflow later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2e16e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pydra.mark.task\n",
    "@pydra.mark.annotate({\"exclusion_patterns\": list, \"n_subjects\":int, \"return\": {\"data_dir\":str}})\n",
    "def get_openneuro_dataset(exclusion_patterns, n_subjects):\n",
    "    \n",
    "    from nilearn.datasets import (fetch_openneuro_dataset_index,\n",
    "                              fetch_openneuro_dataset, select_from_index)\n",
    "    _, urls = fetch_openneuro_dataset_index()\n",
    "    urls = select_from_index(\n",
    "        urls, exclusion_filters=exclusion_patterns, n_subjects = n_subjects)\n",
    "    data_dir, _ = fetch_openneuro_dataset(urls=urls)\n",
    "    return data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14864fdf",
   "metadata": {},
   "source": [
    "### obtain FirstLevelModel objects automatically and fit arguments\n",
    "\n",
    "To get the first level model(s) we have to specify \n",
    "1. the dataset directory\n",
    "2. the task_label\n",
    "3. the space_label \n",
    "4. the folder with the desired derivatives (fMRIPrep)\n",
    "\n",
    "In our case, we only have one subject so we will only have one first level model.\n",
    "Then, for this model, we will obtain \n",
    "1. the list of run images \n",
    "2. events\n",
    "3. confound regressors \n",
    "\n",
    "Those are inferred from the confounds.tsv files available in the BIDS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46247efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pydra.mark.task\n",
    "@pydra.mark.annotate({\"data_dir\": str, \"task_label\": str, \"space_label\": str,\"derivatives_folder\": str, \"smoothing_fwhm\": float, \n",
    "                      \"return\": {\"model\": ty.Any, \"imgs\": list, \"subject\": str}})\n",
    "def get_info_from_bids(\n",
    "    data_dir,\n",
    "    task_label,\n",
    "    space_label,\n",
    "    smoothing_fwhm,\n",
    "    derivatives_folder\n",
    "):\n",
    "    from nilearn.glm.first_level import first_level_from_bids\n",
    "    models, models_run_imgs, models_events, models_confounds = \\\n",
    "    first_level_from_bids(dataset_path = data_dir, task_label = task_label, space_label = space_label,\n",
    "                          smoothing_fwhm = smoothing_fwhm, derivatives_folder = derivatives_folder)\n",
    "    model, imgs, events, confounds = (\n",
    "        models[0], models_run_imgs[0], models_events[0], models_confounds[0])\n",
    "    subject = \"sub-\" + model.subject_label\n",
    "    return model, imgs, subject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c594ddec",
   "metadata": {},
   "source": [
    "### Get design matrix\n",
    "\n",
    "This task does the following:\n",
    "1. read the design matrix in `.mat`\n",
    "2. rename the column\n",
    "3. save the new design matrix as `.csv`\n",
    "\n",
    "**Think:** What if we don't save the new design matrix, but `return` it directly? In other words, we `return` a `pandas.DataFrame` instead of a `path`. What will happen? Worth a try :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d57ec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pydra.mark.task\n",
    "@pydra.mark.annotate({\"data_dir\":str, \"subject\":str, \"return\": {\"dm_path\": str}})\n",
    "def get_designmatrix(data_dir, subject):\n",
    "    \n",
    "    from nilearn.interfaces.fsl import get_design_from_fslmat\n",
    "    fsl_design_matrix_path = os.path.join(\n",
    "        data_dir, 'derivatives', 'task', subject, 'stopsignal.feat', 'design.mat')\n",
    "    design_matrix = get_design_from_fslmat(fsl_design_matrix_path, column_names=None)\n",
    "    \n",
    "    design_columns = ['cond_%02d' % i for i in range(len(design_matrix.columns))]\n",
    "    design_columns[0] = 'Go'\n",
    "    design_columns[4] = 'StopSuccess'\n",
    "    design_matrix.columns = design_columns\n",
    "    dm_path = os.path.join(workflow_out_dir, \"designmatrix.csv\")\n",
    "    design_matrix.to_csv(dm_path,index=None)\n",
    "    return dm_path                      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eb7723",
   "metadata": {},
   "source": [
    "### Fit 1st level model\n",
    "\n",
    "What we are doing here is:\n",
    "1. use the design matrix to fit the first level model\n",
    "2. compute the contrast\n",
    "3. save the z_map and masker for futher use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013219fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pydra.mark.task\n",
    "@pydra.mark.annotate({\"model\": ty.Any,\"imgs\": ty.Any,\n",
    "                      \"dm_path\": ty.Any,\"contrast\": str,\n",
    "                      \"return\": {\"model\": ty.Any, \"z_map_path\":str, \"masker\":ty.Any}})\n",
    "def model_fit(\n",
    "    model, \n",
    "    imgs,\n",
    "    dm_path,\n",
    "    contrast\n",
    "):\n",
    "    import pandas as pd\n",
    "    design_matrix = pd.read_csv(dm_path)\n",
    "    model.fit(imgs, design_matrices = [design_matrix])\n",
    "    z_map = model.compute_contrast(contrast)\n",
    "    z_map_path = os.path.join(workflow_out_dir, \"firstlevel_z_map.nii.gz\")\n",
    "    z_map.to_filename(z_map_path)\n",
    "    masker_path = os.path.join(workflow_out_dir, \"firstlevel_masker.nii.gz\")\n",
    "    masker = model.masker_\n",
    "    return model, z_map_path, masker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f782f61",
   "metadata": {},
   "source": [
    "### Get cluster table and glm report\n",
    "\n",
    "For publication purposes, we obtain a cluster table and a summary report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c83de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pydra.mark.task\n",
    "@pydra.mark.annotate({\"z_map_path\":str,\n",
    "                    \"return\":{\"output_file\":str}})\n",
    "def cluster_table(z_map_path):\n",
    "    import nibabel as nib\n",
    "    from nilearn.reporting import get_clusters_table\n",
    "    from scipy.stats import norm\n",
    "    \n",
    "    stat_img = nib.load(z_map_path)\n",
    "    output_file = os.path.join(workflow_out_dir, \"cluster_table.csv\")\n",
    "    df = get_clusters_table(stat_img,\n",
    "                            stat_threshold = norm.isf(0.001), \n",
    "                            cluster_threshold=10)\n",
    "    df.to_csv(output_file, index=None)\n",
    "    return output_file\n",
    "\n",
    "# get glm report\n",
    "@pydra.mark.task\n",
    "@pydra.mark.annotate({\"model\":ty.Any, \"contrasts\":str,\n",
    "                    \"return\":{\"output_file\":str}})\n",
    "def glm_report(\n",
    "    model,\n",
    "    contrasts\n",
    "):\n",
    "    from nilearn.reporting import make_glm_report\n",
    "    output_file = os.path.join(workflow_out_dir, \"glm_report.html\")\n",
    "    report = make_glm_report(model, contrasts)\n",
    "    report.save_as_html(output_file)\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3971f1",
   "metadata": {},
   "source": [
    "### Make plots \n",
    "\n",
    "Here we want to make some plots to display our results and compare the result from FSL.\n",
    "1. plot nilearn z-map\n",
    "2. plot fsl z-map\n",
    "3. plot nilearn and fsl comparison\n",
    "4. plot design matrix contrast\n",
    "\n",
    "You can also seperate this task into multiple sub-tasks. But it makes more sense to put them into one task as they use the same files and function `nilearn.plotting` repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11643ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pydra.mark.task\n",
    "@pydra.mark.annotate({\"data_dir\":str, \"dm_path\":str, \"z_map_path\":str, \n",
    "                      \"contrast\":str,\"subject\":str, \"masker\":ty.Any, \n",
    "                      \"return\":{\"output_file1\":str, \"output_file2\":str,\n",
    "                                \"output_file3\":str, \"output_file4\":str}})\n",
    "def plots(\n",
    "    data_dir,\n",
    "    dm_path,\n",
    "    z_map_path,\n",
    "    contrast,\n",
    "    subject,\n",
    "    masker,\n",
    "):\n",
    "    import pandas as pd\n",
    "    import nibabel as nib\n",
    "    from nilearn.plotting import plot_glass_brain, plot_img_comparison, plot_contrast_matrix\n",
    "    import matplotlib.pyplot as plt\n",
    "    from scipy.stats import norm\n",
    "    \n",
    "    # plot and save nilearn z-map\n",
    "    z_map = nib.load(z_map_path)\n",
    "    output_file1 = os.path.join(workflow_out_dir, \"nilearn_z_map.png\")\n",
    "    plot_glass_brain(z_map, output_file = output_file1, colorbar = True,\n",
    "                     threshold = norm.isf(0.001), title = 'Nilearn Z map of \"StopSuccess - Go\" (unc p<0.001)',\n",
    "                     plot_abs = False, display_mode = 'ortho')\n",
    "    \n",
    "    # plot and save fsl z-map\n",
    "    fsl_z_map = nib.load(\n",
    "    os.path.join(data_dir, 'derivatives', 'task', subject, 'stopsignal.feat',\n",
    "                 'stats', 'zstat12.nii.gz'))\n",
    "    output_file2 = os.path.join(workflow_out_dir, \"fsl_z_map.png\")\n",
    "    plot_glass_brain(fsl_z_map, output_file = output_file2, colorbar = True, \n",
    "                     threshold = norm.isf(0.001), title = 'FSL Z map of \"StopSuccess - Go\" (unc p<0.001)',\n",
    "                     plot_abs = False, display_mode = 'ortho')\n",
    "    \n",
    "    # plot and save nilearn and fsl comparison\n",
    "    plot_img_comparison([z_map], [fsl_z_map], masker, output_dir = workflow_out_dir, \n",
    "                        ref_label = 'Nilearn', src_label = 'FSL')\n",
    "    old = os.path.join(workflow_out_dir, \"0000.png\")\n",
    "    new = os.path.join(workflow_out_dir, \"nilearn_fsl_comp.png\")\n",
    "    output_file3 = os.rename(old,new)\n",
    "    \n",
    "    # plot and save design matrix contrast\n",
    "    design_matrix = pd.read_csv(dm_path)\n",
    "    output_file4 = os.path.join(workflow_out_dir, \"firstlevel_contrast.png\")\n",
    "    plot_contrast_matrix(contrast, design_matrix, output_file = output_file4)\n",
    "    return output_file1, output_file2, output_file3, output_file4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8d625f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Make a workflow from tasks\n",
    "\n",
    "Now we have created all tasks we need for this first level analysis, and there are two choices for our next step.\n",
    "1. create one workflow to connect all tasks together\n",
    "2. create sub-workflows with some closely related tasks, and connect these workflows along with other tasks into a larger workflow.\n",
    "\n",
    "We recommand the second approach as it is alway a good practice to group tasks, especially when there are a large number of tasks in the analysis.\n",
    "\n",
    "Our analysis can be divided into three parts: (1) get/read the data, (2) analyze the data, and (3) plot the result, where (1) and (3) only have one task each. So we can put all tasks in (2) into one workflow and name it as `firstlevel` or whatever you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfff15d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate a workflow\n",
    "wf_firstlevel = Workflow(name=\"wf_firstlevel\", input_spec=[\"data_dir\",\n",
    "                                     \"task_label\",\n",
    "                                     \"space_label\",\n",
    "                                     \"derivatives_folder\",\n",
    "                                     \"smoothing_fwhm\",\n",
    "                                     \"contrast\",\n",
    "                                     \"output_dir\"]\n",
    "                     )\n",
    "\n",
    "# specify input\n",
    "wf_firstlevel.inputs.task_label = 'stopsignal'\n",
    "wf_firstlevel.inputs.space_label = 'MNI152NLin2009cAsym'\n",
    "wf_firstlevel.inputs.derivatives_folder = 'derivatives/fmriprep'\n",
    "wf_firstlevel.inputs.smoothing_fwhm = 5.0\n",
    "\n",
    "# add task - get_info_from_bids\n",
    "wf_firstlevel.add(get_info_from_bids(name=\"get_info_from_bids\",\n",
    "                          data_dir = wf_firstlevel.lzin.data_dir,\n",
    "                          task_label = wf_firstlevel.lzin.task_label,\n",
    "                          space_label = wf_firstlevel.lzin.space_label,\n",
    "                          derivatives_folder = wf_firstlevel.lzin.derivatives_folder,\n",
    "                          smoothing_fwhm = wf_firstlevel.lzin.smoothing_fwhm\n",
    "                         )\n",
    "      )\n",
    "# add task - get_designmatrix\n",
    "wf_firstlevel.add(get_designmatrix(name = \"get_designmatrix\",\n",
    "                        data_dir = wf_firstlevel.lzin.data_dir,\n",
    "                        subject = wf_firstlevel.get_info_from_bids.lzout.subject,\n",
    "                       )\n",
    "      )\n",
    "wf_firstlevel.add(model_fit(name = \"l1estimation\",\n",
    "                   model = wf_firstlevel.get_info_from_bids.lzout.model, \n",
    "                   imgs = wf_firstlevel.get_info_from_bids.lzout.imgs, \n",
    "                   dm_path = wf_firstlevel.get_designmatrix.lzout.dm_path,\n",
    "                   contrast = wf_firstlevel.lzin.contrast\n",
    "                )\n",
    "      )\n",
    "# add task - cluster_table\n",
    "wf_firstlevel.add(cluster_table(name = \"cluster_table\", \n",
    "                     z_map_path = wf_firstlevel.l1estimation.lzout.z_map_path))\n",
    "# add task - glm_report\n",
    "wf_firstlevel.add(glm_report(name = \"glm_report\",\n",
    "                  model = wf_firstlevel.l1estimation.lzout.model,\n",
    "                  contrasts = wf_firstlevel.lzin.contrast\n",
    "                 )\n",
    "      )\n",
    "# specify output\n",
    "wf_firstlevel.set_output([\n",
    "    (\"z_map\", wf_firstlevel.l1estimation.lzout.z_map_path),\n",
    "    (\"masker\", wf_firstlevel.l1estimation.lzout.masker),\n",
    "    (\"subject\", wf_firstlevel.get_info_from_bids.lzout.subject),\n",
    "    (\"dm_path\", wf_firstlevel.get_designmatrix.lzout.dm_path),\n",
    "    (\"cluster_table\", wf_firstlevel.cluster_table.lzout.output_file),\n",
    "    (\"glm_report\", wf_firstlevel.glm_report.lzout.output_file)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3bcdb1",
   "metadata": {},
   "source": [
    "## The overaching workflow\n",
    "\n",
    "Connect other tasks and the above workflow into one\n",
    "\n",
    "Now we need to create the overaching glm workflow that connects the above workflow and other tasks (e.g., `get/read the data` and `plot the result`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d90299",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = Workflow(name = \"firstlevel_glm\",\n",
    "              input_spec = [\"exclusion_patterns\",\"n_subjects\",\"contrast\",\"output_dir\"],\n",
    "             )\n",
    "\n",
    "wf.inputs.exclusion_patterns = ['*group*', '*phenotype*', '*mriqc*',\n",
    "                                '*parameter_plots*', '*physio_plots*',\n",
    "                                '*space-fsaverage*', '*space-T1w*',\n",
    "                                '*dwi*', '*beh*', '*task-bart*',\n",
    "                                '*task-rest*', '*task-scap*', '*task-task*']\n",
    "wf.inputs.n_subjects = 1\n",
    "wf.inputs.output_dir = workflow_out_dir\n",
    "wf.inputs.contrast = 'StopSuccess - Go'\n",
    "\n",
    "wf.add(get_openneuro_dataset(name = \"get_openneuro_dataset\", \n",
    "                             exclusion_patterns = wf.lzin.exclusion_patterns,\n",
    "                             n_subjects = wf.lzin.n_subjects\n",
    "                            )\n",
    "      )\n",
    "\n",
    "wf_firstlevel.inputs.data_dir = wf.get_openneuro_dataset.lzout.data_dir\n",
    "wf_firstlevel.inputs.contrast = wf.inputs.contrast\n",
    "wf_firstlevel.inputs.output_dir = wf.inputs.output_dir\n",
    "wf.add(wf_firstlevel)\n",
    "\n",
    "wf.add(plots(name = \"plots\",\n",
    "             data_dir = wf.get_openneuro_dataset.lzout.data_dir,\n",
    "             dm_path = wf_firstlevel.lzout.dm_path,\n",
    "             z_map_path = wf_firstlevel.lzout.z_map,\n",
    "             contrast = wf.lzin.contrast,\n",
    "             subject =  wf_firstlevel.lzout.subject,\n",
    "             masker = wf_firstlevel.lzout.masker\n",
    "            )\n",
    "      )\n",
    "\n",
    "wf.set_output([\n",
    "    (\"output1\", wf.plots.lzout.output_file1),\n",
    "    (\"output2\", wf.plots.lzout.output_file2),\n",
    "    (\"output3\", wf.plots.lzout.output_file3),\n",
    "    (\"output4\", wf.plots.lzout.output_file4)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19576646",
   "metadata": {},
   "source": [
    "## Run Workflow Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f66cb94",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "from pydra import Submitter\n",
    "\n",
    "with Submitter(plugin=\"cf\", n_procs=4) as submitter:\n",
    "    submitter(wf)\n",
    "\n",
    "results = wf.result()\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb07e53",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971ab67a",
   "metadata": {},
   "source": [
    "If you arrive here without any errors, yay, you just made your first pydra workflow for a first-level GLM!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9344429f",
   "metadata": {},
   "source": [
    "## Examine folder structure\n",
    "\n",
    "Let's take a look at what you have got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1793503",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../outputs/6_glm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ce24ad",
   "metadata": {},
   "source": [
    "<details>\n",
    "    \n",
    "<summary>Click to see what you should get</summary>\n",
    "    \n",
    "1. cluster_table.csv       \n",
    "2. firstlevel_z_map.nii.gz \n",
    "3. nilearn_fsl_comp.png\n",
    "4. designmatrix.csv        \n",
    "5. fsl_z_map.png           \n",
    "6. nilearn_z_map.png\n",
    "7. firstlevel_contrast.png glm_report.html\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2d57f4",
   "metadata": {},
   "source": [
    "### Plot figures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e98820",
   "metadata": {},
   "source": [
    "#### First level contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4450084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='../outputs/6_glm/firstlevel_contrast.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8a5ea3",
   "metadata": {},
   "source": [
    "#### Nilearn Z map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8946cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../outputs/6_glm/nilearn_z_map.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6b0a26",
   "metadata": {},
   "source": [
    "#### FSL Z map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b26d4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../outputs/6_glm/fsl_z_map.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d31e714",
   "metadata": {},
   "source": [
    "#### Nilearn FSL comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a9dd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../outputs/6_glm/nilearn_fsl_comp.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8e8301",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c97d28",
   "metadata": {},
   "source": [
    "What if we need to run the first-level GLM on multiple subject? We will need the `splitter`. \n",
    "\n",
    "So, where should we add `.split`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67c759a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.10.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   14,
   18,
   24,
   29,
   35,
   54,
   65,
   77,
   89,
   107,
   126,
   137,
   154,
   163,
   183,
   189,
   219,
   231,
   281,
   293,
   352,
   360,
   401,
   405,
   418,
   422,
   426,
   432,
   434,
   450,
   454,
   458,
   461,
   465,
   467,
   471,
   473,
   477,
   481,
   485,
   491
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}